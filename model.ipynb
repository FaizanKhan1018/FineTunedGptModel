{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8897203,"sourceType":"datasetVersion","datasetId":5349473}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import wandb\nfrom datasets import load_dataset\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling","metadata":{"execution":{"iopub.status.busy":"2024-07-08T04:13:32.620725Z","iopub.execute_input":"2024-07-08T04:13:32.621083Z","iopub.status.idle":"2024-07-08T04:13:52.301198Z","shell.execute_reply.started":"2024-07-08T04:13:32.621053Z","shell.execute_reply":"2024-07-08T04:13:52.300212Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-07-08 04:13:42.786927: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-08 04:13:42.787026: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-08 04:13:42.918959: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"wandb.init(project=\"gpt2-fine-tuning\", name=\"fine-tune-gpt2\")\nprint(\"done..\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-08T04:13:52.303343Z","iopub.execute_input":"2024-07-08T04:13:52.304102Z","iopub.status.idle":"2024-07-08T04:14:27.663641Z","shell.execute_reply.started":"2024-07-08T04:13:52.304066Z","shell.execute_reply":"2024-07-08T04:14:27.662709Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240708_041410-ogax7029</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/fk555951/gpt2-fine-tuning/runs/ogax7029' target=\"_blank\">fine-tune-gpt2</a></strong> to <a href='https://wandb.ai/fk555951/gpt2-fine-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/fk555951/gpt2-fine-tuning' target=\"_blank\">https://wandb.ai/fk555951/gpt2-fine-tuning</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/fk555951/gpt2-fine-tuning/runs/ogax7029' target=\"_blank\">https://wandb.ai/fk555951/gpt2-fine-tuning/runs/ogax7029</a>"},"metadata":{}},{"name":"stdout","text":"done..\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load tokenizer and add padding token if not present\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\nprint(\"done\")","metadata":{"execution":{"iopub.status.busy":"2024-07-08T04:14:27.664838Z","iopub.execute_input":"2024-07-08T04:14:27.665174Z","iopub.status.idle":"2024-07-08T04:14:28.672212Z","shell.execute_reply.started":"2024-07-08T04:14:27.665147Z","shell.execute_reply":"2024-07-08T04:14:28.670967Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5aabbe858a7d49e8bcec677c4512c841"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"454ba4cc657e4e6e8aaa7c7d7ed1e6ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ae3bb7e68c74737836abdbcd72ac4d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28822aeedb0a4cd99b9e55a16576adaf"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0001b65ac41419f9c46db635260e88f"}},"metadata":{}},{"name":"stdout","text":"done\n","output_type":"stream"}]},{"cell_type":"code","source":"# Defining function to load a subset of the dataset\ndef load_subset_dataset(dataset_path, num_samples=100):\n    dataset = load_dataset('arrow', data_files={\"train\": dataset_path})\n    return dataset['train'].shuffle().select(range(num_samples))\nprint(\"donee\")","metadata":{"execution":{"iopub.status.busy":"2024-07-08T04:14:28.675055Z","iopub.execute_input":"2024-07-08T04:14:28.675443Z","iopub.status.idle":"2024-07-08T04:14:28.683848Z","shell.execute_reply.started":"2024-07-08T04:14:28.675411Z","shell.execute_reply":"2024-07-08T04:14:28.682640Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"donee\n","output_type":"stream"}]},{"cell_type":"code","source":"# Tokenization function\ndef tokenize_function(examples):\n    tokenized_inputs = tokenizer(examples[\"inputs_pretokenized\"], padding=\"max_length\", truncation=True, max_length=128)\n    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n    return tokenized_inputs\nprint(\"done\")","metadata":{"execution":{"iopub.status.busy":"2024-07-08T04:14:28.685408Z","iopub.execute_input":"2024-07-08T04:14:28.685930Z","iopub.status.idle":"2024-07-08T04:14:28.693919Z","shell.execute_reply.started":"2024-07-08T04:14:28.685893Z","shell.execute_reply":"2024-07-08T04:14:28.692940Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"done\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load and preprocess a subset of the dataset\ndataset_path = '/kaggle/input/intern-arrow/data-00000-of-00001.arrow'\ntokenized_train_dataset = load_subset_dataset(dataset_path, num_samples=500)\n\n# Tokenize dataset\ntokenized_train_dataset = tokenized_train_dataset.map(tokenize_function, batched=True)\n\n# Prepare data collator to handle data augmentation for language modeling\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T04:14:28.695276Z","iopub.execute_input":"2024-07-08T04:14:28.696405Z","iopub.status.idle":"2024-07-08T04:14:32.498500Z","shell.execute_reply.started":"2024-07-08T04:14:28.696365Z","shell.execute_reply":"2024-07-08T04:14:32.497410Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73d55b91d2e04c698501c9e96c191eee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c59f51c522b74638b176dcc102af146b"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load model and adjust for added tokens\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2024-07-08T04:14:32.500095Z","iopub.execute_input":"2024-07-08T04:14:32.500489Z","iopub.status.idle":"2024-07-08T04:14:38.010510Z","shell.execute_reply.started":"2024-07-08T04:14:32.500456Z","shell.execute_reply":"2024-07-08T04:14:38.009526Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73800660a1f1482ea8844c3fc8d315cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e0a301ea5df448fb1517a1c183c1b0a"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Embedding(50258, 768)"},"metadata":{}}]},{"cell_type":"code","source":"# Load and preprocess a subset of the evaluation dataset\neval_dataset_path = '/kaggle/input/intern-arrow/data-00000-of-00001.arrow'\ntokenized_eval_dataset = load_subset_dataset(eval_dataset_path, num_samples=10)\n\n# Tokenize evaluation dataset\ntokenized_eval_dataset = tokenized_eval_dataset.map(tokenize_function, batched=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-08T04:14:38.011830Z","iopub.execute_input":"2024-07-08T04:14:38.012204Z","iopub.status.idle":"2024-07-08T04:14:40.460037Z","shell.execute_reply.started":"2024-07-08T04:14:38.012171Z","shell.execute_reply":"2024-07-08T04:14:40.459019Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd71a7424a22465aad6758b8de3b904d"}},"metadata":{}}]},{"cell_type":"code","source":"# Defining training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    overwrite_output_dir=True,\n    num_train_epochs=5,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    logging_steps=500,\n    save_steps=1000,\n    evaluation_strategy=\"steps\",\n    logging_dir=\"./logs\",\n    report_to=\"wandb\"  # Enable logging to Weights and Biases\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-08T04:14:40.461380Z","iopub.execute_input":"2024-07-08T04:14:40.461770Z","iopub.status.idle":"2024-07-08T04:14:40.590802Z","shell.execute_reply.started":"2024-07-08T04:14:40.461734Z","shell.execute_reply":"2024-07-08T04:14:40.589599Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize Trainer\n# Initialize Trainer with both training and evaluation datasets\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_eval_dataset,  # Specify the eval dataset here\n    data_collator=data_collator,\n)\n\n# Train the model\ntrainer.train()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-08T04:14:40.594170Z","iopub.execute_input":"2024-07-08T04:14:40.594499Z","iopub.status.idle":"2024-07-08T04:16:56.304027Z","shell.execute_reply.started":"2024-07-08T04:14:40.594472Z","shell.execute_reply":"2024-07-08T04:16:56.303132Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [625/625 02:13, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>3.109500</td>\n      <td>1.968977</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=625, training_loss=2.8420236572265627, metrics={'train_runtime': 135.1042, 'train_samples_per_second': 18.504, 'train_steps_per_second': 4.626, 'total_flos': 163307520000000.0, 'train_loss': 2.8420236572265627, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"code","source":"output_dir = \"./fine-tuned-gpt2\"\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-08T04:16:56.305377Z","iopub.execute_input":"2024-07-08T04:16:56.305682Z","iopub.status.idle":"2024-07-08T04:16:57.417089Z","shell.execute_reply.started":"2024-07-08T04:16:56.305657Z","shell.execute_reply":"2024-07-08T04:16:57.415138Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"('./fine-tuned-gpt2/tokenizer_config.json',\n './fine-tuned-gpt2/special_tokens_map.json',\n './fine-tuned-gpt2/vocab.json',\n './fine-tuned-gpt2/merges.txt',\n './fine-tuned-gpt2/added_tokens.json')"},"metadata":{}}]},{"cell_type":"code","source":"# Finish the Weights and Biases run\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-07-08T04:16:57.418176Z","iopub.execute_input":"2024-07-08T04:16:57.421051Z","iopub.status.idle":"2024-07-08T04:17:01.114485Z","shell.execute_reply.started":"2024-07-08T04:16:57.421015Z","shell.execute_reply":"2024-07-08T04:17:01.113776Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁█</td></tr><tr><td>train/global_step</td><td>▁▁█</td></tr><tr><td>train/grad_norm</td><td>▁</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.96898</td></tr><tr><td>eval/runtime</td><td>0.2154</td></tr><tr><td>eval/samples_per_second</td><td>46.432</td></tr><tr><td>eval/steps_per_second</td><td>13.93</td></tr><tr><td>total_flos</td><td>163307520000000.0</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>625</td></tr><tr><td>train/grad_norm</td><td>7.87769</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>3.1095</td></tr><tr><td>train_loss</td><td>2.84202</td></tr><tr><td>train_runtime</td><td>135.1042</td></tr><tr><td>train_samples_per_second</td><td>18.504</td></tr><tr><td>train_steps_per_second</td><td>4.626</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fine-tune-gpt2</strong> at: <a href='https://wandb.ai/fk555951/gpt2-fine-tuning/runs/ogax7029' target=\"_blank\">https://wandb.ai/fk555951/gpt2-fine-tuning/runs/ogax7029</a><br/> View project at: <a href='https://wandb.ai/fk555951/gpt2-fine-tuning' target=\"_blank\">https://wandb.ai/fk555951/gpt2-fine-tuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240708_041410-ogax7029/logs</code>"},"metadata":{}}]}]}